{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944c58ab",
   "metadata": {},
   "source": [
    "# Fine-tuning on Pre-trained Model for Multiomic Integration\n",
    "In this tutorial, we demonstrate how to fine-tune a pre-trained model for multiomic integration. We use the BMMC dataset as an example for RNA and protein abundance data integration from CITE-seq. The BMMC dataset consists of multiple batches by donor origin. In the paired integration setting, each cell contains both RNA and protein measurements. We fine-tune on the pre-trained whole-body model.\n",
    "\n",
    "We summarize the fine-tuning pipeline in the following steps, which can be used as a general recipe for finetuning on integration tasks and beyond: \n",
    "\n",
    "     1. Specify hyper-parameter setup for integration task\n",
    "     \n",
    "     2. Load and pre-process data\n",
    "     \n",
    "     3. Load the pre-trained scGPT model\n",
    "     \n",
    "     4. Finetune scGPT with task-specific objectives\n",
    "     \n",
    "     5. Evaluate fine-tuned scGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "190fb7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/.venv/lib/python3.12/site-packages/torchtext/__init__.py:7: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \"\\n/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \\n\"\n",
      "/app/.venv/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/app/.venv/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/app/.venv/lib/python3.12/site-packages/scgpt/model/model.py:21: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/app/.venv/lib/python3.12/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/app/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "#from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "# import scvi\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn import preprocessing\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from scgpt import prepare_data, prepare_dataloader, define_wandb_metrcis, evaluate, eval_testdata, train\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch\n",
    "from scgpt.model import MultiOmicTransformerModel\n",
    "\n",
    "import scgpt as scg\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.tokenizer import random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(4, 4))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d857987-90a9-4f1a-bcf1-97be63b6c966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded libcuda.so.1 OK\n",
      "cuInit returned: 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ctypes\n",
    "try:\n",
    "    cuda = ctypes.CDLL('libcuda.so.1')\n",
    "    print('Loaded libcuda.so.1 OK')\n",
    "    err = cuda.cuInit(0)\n",
    "    print(f'cuInit returned: {err}')\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3931b62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step1: Specify hyper-parameter setup for integration task\n",
    "Here we provide some hyper-parameter recommendations for the multiomic integration task. Note that the BMMC dataset contains multiple batches to be integrated. Therefore, in addition to the default gene modelling objectives, we also turn on DAR objectives specifically to faciliate batch integration. We also turn on the use_mod argument as default to ensure that the model is modality-aware during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e9da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    task = 'multiomic',\n",
    "    seed=42,\n",
    "    dataset_name=\"BMMC\", # Dataset name\n",
    "    do_train=True, # Flag to indicate whether to do update model parameters during training\n",
    "    load_model=\"../models/scGPT_human\", # Path to pre-trained model\n",
    "    freeze = False, #freeze\n",
    "    GEP=True, # Gene expression modelling\n",
    "    GEPC=True, # Gene expression modelling for cell objective\n",
    "    CLS=False,\n",
    "    ESC=False,\n",
    "    DAR = True, # DAR objective weight for batch correction\n",
    "    DSBN = False,  # Domain-spec batchnorm,\n",
    "    mask_ratio=0.4, # Default mask ratio\n",
    "    explicit_zero_prob = False,  # whether explicit bernoulli for zeros\n",
    "    ecs_thres=0,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=1.0,\n",
    "    use_batch_labels = True,\n",
    "    use_mod = True,\n",
    "    per_seq_batch_sample = False,\n",
    "    epochs=25, # Default number of epochs for fine-tuning\n",
    "    input_layer_key = \"X_binned\", # Default expression value binning in data pre-processing\n",
    "    n_bins=51, # Default number of bins for value binning in data pre-processing\n",
    "    n_hvg = 1200,  # Default number of highly variable genes\n",
    "    n_hvp = 4000,\n",
    "    max_seq_len = 4001, # # Default n_hvg+1\n",
    "    lr=1e-3, # Default learning rate for fine-tuning\n",
    "    batch_size=16, # Default batch size for fine-tuning\n",
    "    layer_size=512,\n",
    "    nlayers=4,\n",
    "    nhead=8, # if load model, batch_size, layer_size, nlayers, nhead will be ignored\n",
    "    dropout=0.2, # Default dropout rate during model fine-tuning\n",
    "    schedule_ratio=0.95,  # Default rate for learning rate decay\n",
    "    save_eval_interval=5, # Default model evaluation interval\n",
    "    log_interval=100, # Default log interval\n",
    "    fast_transformer=False, # Default setting\n",
    "    pre_norm=False, # Default setting\n",
    "    amp=True,  # Default setting: Automatic Mixed Precision\n",
    "    pad_token = \"<pad>\",\n",
    "    mask_value = -1,\n",
    "    pad_value = -2,\n",
    "    include_zero_gene = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e7e294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using W&B in offline mode.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/app/notebooks/wandb/offline-run-20260205_033430-qxmq2pvc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'multiomic', 'seed': 42, 'dataset_name': 'BMMC', 'do_train': True, 'load_model': '../models/scGPT_human', 'freeze': False, 'GEP': True, 'GEPC': True, 'CLS': False, 'ESC': False, 'DAR': True, 'DSBN': False, 'mask_ratio': 0.4, 'explicit_zero_prob': False, 'ecs_thres': 0, 'dab_weight': 1, 'use_batch_labels': True, 'use_mod': True, 'per_seq_batch_sample': False, 'epochs': 25, 'input_layer_key': 'X_binned', 'n_bins': 51, 'n_hvg': 1200, 'n_hvp': 4000, 'max_seq_len': 4001, 'lr': 0.001, 'batch_size': 16, 'layer_size': 512, 'nlayers': 4, 'nhead': 8, 'dropout': 0.2, 'schedule_ratio': 0.95, 'save_eval_interval': 5, 'log_interval': 100, 'fast_transformer': False, 'pre_norm': False, 'amp': True, 'pad_token': '<pad>', 'mask_value': -1, 'pad_value': -2, 'include_zero_gene': False}\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"scGPT\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "327a9ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for input and preprocessing\n",
    "special_tokens = [config.pad_token, \"<cls>\", \"<eoc>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf30635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to ../data/dev_BMMC-Feb05-03-34\n"
     ]
    }
   ],
   "source": [
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"../data/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26f7516",
   "metadata": {},
   "source": [
    "## Step 2: Load and pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653e6b7",
   "metadata": {},
   "source": [
    "### 2.1 Load the BMMC data\n",
    "Please download the data from https://drive.google.com/file/d/10RxboePS5p2Jj2Sfq1Ghzgqnl6nqPv5V/view?usp=sharing\n",
    "\n",
    "For the purpose of this tutorial, we selected a 12K+ cell subset from the BMMC dataset (originally 90K+ cells) to speed up training. More specifically, we filtered on the first 3 donors (batches) with B, Mono and T cell subtypes as shown in the data loading step below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14f8edf6-4c5d-4056-8954-c11c737a50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'BMMC':\n",
    "    adata = sc.read('../data/BMMC_processed.h5ad')\n",
    "    # subset to first 3 donors with B, Mono and T cell subtypes\n",
    "    adata = adata[adata.obs.DonorID.isin([10886, 11466, 12710]) & adata.obs.cell_type.isin(np.unique(adata.obs.cell_type.values)[:17])]\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"cell_type\"].astype(str).astype('category')\n",
    "    adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    encoded_batch = le.fit_transform(adata.obs['batch'].values)\n",
    "    adata.obs[\"batch_id\"] =  encoded_batch\n",
    "    adata.obs[\"str_batch\"] = adata.obs[\"batch_id\"].astype('category')\n",
    "    adata_protein = adata[:, adata.var.feature_types.isin(['ADT'])].copy()\n",
    "    adata_protein.var.index = ['p_' + i for i in adata_protein.var.index]\n",
    "    adata = adata[:, adata.var.feature_types.isin(['GEX'])].copy()\n",
    "    data_is_raw = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bfe331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.use_mod:\n",
    "    gene_rna_df = pd.DataFrame(index = adata.var.index.tolist())\n",
    "    gene_rna_df['mod'] = 'RNA'\n",
    "    gene_protein_df = pd.DataFrame(index = adata_protein.var.index.tolist())\n",
    "    gene_protein_df['mod'] = 'Protein'\n",
    "    gene_loc_df = pd.concat([gene_rna_df, gene_protein_df])\n",
    "    gene_loc_df['mod'] = gene_loc_df['mod'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7371ebc",
   "metadata": {},
   "source": [
    "### 2.2 Cross-check gene set with the pre-trained model \n",
    "Note that we retain the common gene set between the data and the pre-trained model for further fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cc7d1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 12587/13953 genes in vocabulary of size 60697.\n"
     ]
    }
   ],
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    old_vocab = vocab\n",
    "embsize = config.layer_size\n",
    "nhead = config.nhead\n",
    "nlayers = config.nlayers\n",
    "d_hid = config.layer_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f414d",
   "metadata": {},
   "source": [
    "### 2.3 Pre-process the data\n",
    "We follow the standardized pipline of depth normalization, log normalization, and highly vairable gene (HVG) selection for data pre-processing. We further introduce value binning to obtain the relative expressions of each HVG. Given multiple sequencing modalities, we perform the pre-processing steps on each individual modality first, and then combine them into multi-modal sequences as model input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32228422",
   "metadata": {},
   "source": [
    "#### 2.3.1 Pre-process the RNA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69be564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering genes by counts ...\n",
      "scGPT - INFO - Filtering cells by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Subsetting highly variable genes ...\n",
      "scGPT - WARNING - No batch_key is provided, will use all cells for HVG selection.\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=1,  # step 1\n",
    "    filter_cell_by_counts=1,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=config.n_hvg,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor(adata, batch_key=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12efe68e",
   "metadata": {},
   "source": [
    "#### 2.3.2 Pre-process the Protein data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72dc9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "preprocessor_protein = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=0,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=False,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=False,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=None,\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor_protein(adata_protein, batch_key=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e11257",
   "metadata": {},
   "source": [
    "#### 2.3.3 Combine RNA, and Protein data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ab9f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = np.concatenate([adata.layers[\"X_binned\"], adata_protein.layers[\"X_binned\"]], axis=1)\n",
    "adata = AnnData(\n",
    "    X=data_combined,\n",
    "    obs=adata.obs,\n",
    "    var=pd.DataFrame(index=adata.var_names.tolist() + adata_protein.var_names.tolist()),\n",
    "    layers={\"X_binned\": data_combined,}\n",
    ")\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f717788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 12578 × 1334\n",
       "    obs: 'GEX_n_genes_by_counts', 'GEX_pct_counts_mt', 'GEX_size_factors', 'GEX_phase', 'ADT_n_antibodies_by_counts', 'ADT_total_counts', 'ADT_iso_count', 'cell_type', 'batch', 'ADT_pseudotime_order', 'GEX_pseudotime_order', 'Samplename', 'Site', 'DonorNumber', 'Modality', 'VendorLot', 'DonorID', 'DonorAge', 'DonorBMI', 'DonorBloodType', 'DonorRace', 'Ethnicity', 'DonorGender', 'QCMeds', 'DonorSmoker', 'is_train', 'celltype', 'batch_id', 'str_batch', 'n_counts'\n",
       "    var: 'gene_name'\n",
       "    layers: 'X_binned'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "372f68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.per_seq_batch_sample:\n",
    "    # sort the adata by batch_id in advance\n",
    "    adata_sorted = adata[adata.obs[\"batch_id\"].argsort()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c2a1c",
   "metadata": {},
   "source": [
    "### 2.4 Tokenize the input data for model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "116c8c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counts = (\n",
    "    adata.layers[config.input_layer_key].toarray()\n",
    "    if issparse(adata.layers[config.input_layer_key])\n",
    "    else adata.layers[config.input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "celltypes_labels = adata.obs[\"celltype\"].tolist()  # make sure count from 0\n",
    "num_types = len(set(celltypes_labels))\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9ac9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.use_mod:\n",
    "    mod_type = np.array([gene_loc_df.loc[g, 'mod'] for g in genes])\n",
    "    vocab_mod = Vocab(VocabPybind(np.unique(gene_loc_df['mod']).tolist() + special_tokens, None))\n",
    "    vocab_mod.set_default_index(vocab_mod[\"<pad>\"])\n",
    "    mod_type = np.array(vocab_mod(list(mod_type)), dtype=int)\n",
    "    ntokens_mod = len(vocab_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "753b6a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_celltype_labels,\n",
    "    valid_celltype_labels,\n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    ") = train_test_split(\n",
    "    all_counts, celltypes_labels, batch_ids, test_size=0.1, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a942b56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max num of non_zero genes: 687\n",
      "min num of non_zero genes: 206\n",
      "average num of non_zero genes: 341.5295053003534\n",
      "99% quantile num of non_zero genes: 510.0\n",
      "max original values: 50\n",
      "average original non_zero values: 25.3482856428962\n",
      "99% quantile original non_zero values: 49.0\n",
      "num of celltypes: 17\n"
     ]
    }
   ],
   "source": [
    "num_of_non_zero_genes = [\n",
    "    np.count_nonzero(train_data[i]) for i in range(train_data.shape[0])\n",
    "]\n",
    "print(f\"max num of non_zero genes: {np.max(num_of_non_zero_genes)}\")\n",
    "print(f\"min num of non_zero genes: {np.min(num_of_non_zero_genes)}\")\n",
    "print(f\"average num of non_zero genes: {np.mean(num_of_non_zero_genes)}\")\n",
    "print(\n",
    "    f\"99% quantile num of non_zero genes: {np.quantile(num_of_non_zero_genes, 0.99)}\"\n",
    ")\n",
    "print(f\"max original values: {np.max(train_data)}\")\n",
    "print(\n",
    "    f\"average original non_zero values: {np.mean(train_data[np.nonzero(train_data)])}\"\n",
    ")\n",
    "print(\n",
    "    f\"99% quantile original non_zero values: {np.quantile(train_data[np.nonzero(train_data)], 0.99)}\"\n",
    ")\n",
    "print(f\"num of celltypes: {num_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fd4b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.load_model is None:\n",
    "    vocab = Vocab(VocabPybind(genes + special_tokens, None))\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    gene_ids = np.array(vocab(genes), dtype=int)\n",
    "else:\n",
    "    pretrained_genes = [g for g in genes + special_tokens if g in old_vocab]\n",
    "    new_genes = [g for g in genes + special_tokens if g not in old_vocab]\n",
    "    gene_ids_pretrained = np.array(old_vocab(pretrained_genes), dtype=int)\n",
    "    # https://discuss.pytorch.org/t/expand-an-existing-embedding-and-linear-layer-nan-loss-value/55670/2\n",
    "    # Retrieve pretrained weights\n",
    "    vocab = Vocab(VocabPybind(pretrained_genes + new_genes, None))\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b578ef0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 11320, \n",
      "\t feature length: 688\n",
      "scGPT - INFO - valid set number of samples: 1258, \n",
      "\t feature length: 663\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=config.max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=config.pad_token,\n",
    "    pad_value=config.pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=config.include_zero_gene,\n",
    "    mod_type=mod_type if config.use_mod else None,\n",
    "    vocab_mod=vocab_mod if config.use_mod else None,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=config.max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=config.pad_token,\n",
    "    pad_value=config.pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=config.include_zero_gene,\n",
    "    mod_type=mod_type if config.use_mod else None,\n",
    "    vocab_mod=vocab_mod if config.use_mod else None,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f78ec",
   "metadata": {},
   "source": [
    "## Step 3: Load the pre-trained scGPT model\n",
    "Note that for multiomic integration, since the pre-trained model does not include the ATAC and protein tokens, we expand the embedding layer by adding these new tokens. We inherit only the gene embedding layer from the pre-trained model, and train rest of the model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d57aaf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model_dict = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m ntokens = \u001b[38;5;28mlen\u001b[39m(vocab)  \u001b[38;5;66;03m# size of vocabulary\u001b[39;00m\n\u001b[32m      4\u001b[39m model = MultiOmicTransformerModel(\n\u001b[32m      5\u001b[39m     ntokens,\n\u001b[32m      6\u001b[39m     embsize,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     vocab_mod=vocab_mod \u001b[38;5;28;01mif\u001b[39;00m config.use_mod \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.12/site-packages/torch/serialization.py:1025\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1023\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1024\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(UNSAFE_MESSAGE + \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[32m   1031\u001b[39m     f_name = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.12/site-packages/torch/serialization.py:1446\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   1444\u001b[39m unpickler = UnpicklerWrapper(data_file, **pickle_load_args)\n\u001b[32m   1445\u001b[39m unpickler.persistent_load = persistent_load\n\u001b[32m-> \u001b[39m\u001b[32m1446\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1448\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n\u001b[32m   1449\u001b[39m torch._C._log_api_usage_metadata(\n\u001b[32m   1450\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtorch.load.metadata\u001b[39m\u001b[33m\"\u001b[39m, {\u001b[33m\"\u001b[39m\u001b[33mserialization_id\u001b[39m\u001b[33m\"\u001b[39m: zip_file.serialization_id()}\n\u001b[32m   1451\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.12/site-packages/torch/serialization.py:1416\u001b[39m, in \u001b[36m_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   1414\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1415\u001b[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1416\u001b[39m     typed_storage = \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1418\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.12/site-packages/torch/serialization.py:1390\u001b[39m, in \u001b[36m_load.<locals>.load_tensor\u001b[39m\u001b[34m(dtype, numel, key, location)\u001b[39m\n\u001b[32m   1385\u001b[39m         storage.byteswap(dtype)\n\u001b[32m   1387\u001b[39m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[32m   1388\u001b[39m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[32m   1389\u001b[39m typed_storage = torch.storage.TypedStorage(\n\u001b[32m-> \u001b[39m\u001b[32m1390\u001b[39m     wrap_storage=\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1391\u001b[39m     dtype=dtype,\n\u001b[32m   1392\u001b[39m     _internal=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typed_storage._data_ptr() != \u001b[32m0\u001b[39m:\n\u001b[32m   1395\u001b[39m     loaded_storages[key] = typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.12/site-packages/torch/serialization.py:390\u001b[39m, in \u001b[36mdefault_restore_location\u001b[39m\u001b[34m(storage, location)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_restore_location\u001b[39m(storage, location):\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    392\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.12/site-packages/torch/serialization.py:265\u001b[39m, in \u001b[36m_cuda_deserialize\u001b[39m\u001b[34m(obj, location)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m location.startswith(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m         device = \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[33m\"\u001b[39m\u001b[33m_torch_load_uninitialized\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    267\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.device(device):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.12/site-packages/torch/serialization.py:249\u001b[39m, in \u001b[36mvalidate_cuda_device\u001b[39m\u001b[34m(location)\u001b[39m\n\u001b[32m    246\u001b[39m device = torch.cuda._utils._get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.cuda.is_available():\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mAttempting to deserialize object on a CUDA \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    250\u001b[39m                        \u001b[33m'\u001b[39m\u001b[33mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    251\u001b[39m                        \u001b[33m'\u001b[39m\u001b[33mIf you are running on a CPU-only machine, \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    252\u001b[39m                        \u001b[33m'\u001b[39m\u001b[33mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    253\u001b[39m                        \u001b[33m'\u001b[39m\u001b[33mto map your storages to the CPU.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    254\u001b[39m device_count = torch.cuda.device_count()\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device >= device_count:\n",
      "\u001b[31mRuntimeError\u001b[39m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dict = torch.load(model_file)\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = MultiOmicTransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid, \n",
    "    nlayers,\n",
    "    vocab=vocab,\n",
    "    dropout=config.dropout,\n",
    "    pad_token=config.pad_token,\n",
    "    pad_value=config.pad_value,\n",
    "    do_mvc=config.GEPC,\n",
    "    do_dab=config.DAR,\n",
    "    use_batch_labels=config.use_batch_labels,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=config.DSBN,\n",
    "    n_input_bins=config.n_bins,\n",
    "    ecs_threshold=config.ecs_thres,\n",
    "    explicit_zero_prob=config.explicit_zero_prob,\n",
    "    use_fast_transformer=config.fast_transformer,\n",
    "    pre_norm=config.pre_norm,\n",
    "    use_mod=config.use_mod,\n",
    "    ntokens_mod=ntokens_mod if config.use_mod else None,\n",
    "    vocab_mod=vocab_mod if config.use_mod else None,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    pretrained_emb_weights = model_dict['encoder.embedding.weight'][gene_ids_pretrained, :]\n",
    "    model.encoder.embedding.weight.data[:len(pretrained_genes), :] = pretrained_emb_weights\n",
    "    model.encoder.enc_norm.weight.data = model_dict['encoder.enc_norm.weight']\n",
    "ntokens = len(vocab)\n",
    "\n",
    "model.to(device)\n",
    "wandb.watch(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.GEP and config.GEPC:\n",
    "    criterion_gep_gepc = masked_mse_loss\n",
    "if config.CLS:\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "if config.DAR:\n",
    "    criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4d9e08",
   "metadata": {},
   "source": [
    "## Step 4: Finetune scGPT with task-specific objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c9cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "define_wandb_metrcis()\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_data_pt, valid_data_pt = prepare_data(\n",
    "        tokenized_train=tokenized_train, \n",
    "        tokenized_valid=tokenized_valid, \n",
    "        train_batch_labels=train_batch_labels,\n",
    "        valid_batch_labels=valid_batch_labels,\n",
    "        config=config,\n",
    "        epoch=epoch,\n",
    "        sort_seq_batch=config.per_seq_batch_sample)\n",
    "    \n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "        per_seq_batch_sample=config.per_seq_batch_sample\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "        per_seq_batch_sample=config.per_seq_batch_sample\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(\n",
    "            model=model,\n",
    "            loader=train_loader,\n",
    "            vocab=vocab,\n",
    "            criterion_gep_gepc=criterion_gep_gepc if config.GEP and config.GEPC else None,\n",
    "            criterion_dab=criterion_dab if config.DAR else None,\n",
    "            criterion_cls=criterion_cls if config.CLS else None,\n",
    "            scaler=scaler,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            config=config,\n",
    "            logger=logger,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "    val_loss = evaluate(\n",
    "        model=model,\n",
    "        loader=valid_loader,\n",
    "        vocab=vocab,\n",
    "        criterion_gep_gepc=criterion_gep_gepc if config.GEP and config.GEPC else None,\n",
    "        criterion_dab=criterion_dab if config.DAR else None,\n",
    "        criterion_cls=criterion_cls if config.CLS else None,\n",
    "        device=device,\n",
    "        config=config,\n",
    "        epoch=epoch\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss {val_loss:5.4f} | \"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "    if epoch % config.save_eval_interval == 0 or epoch == config.epochs:\n",
    "        logger.info(f\"Saving model to {save_dir}\")\n",
    "        torch.save(best_model.state_dict(), save_dir / f\"model_e{best_model_epoch}.pt\")\n",
    "\n",
    "        # eval on testdata\n",
    "        results = eval_testdata(\n",
    "            model = best_model,\n",
    "            adata_t = adata_sorted if config.per_seq_batch_sample else adata,\n",
    "            gene_ids = gene_ids,\n",
    "            vocab = vocab,\n",
    "            config = config,\n",
    "            logger = logger,\n",
    "            include_types=[\"cls\"],\n",
    "        )\n",
    "        results[\"batch_umap\"].savefig(\n",
    "            save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "        )\n",
    "\n",
    "        results[\"celltype_umap\"].savefig(\n",
    "            save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "        )\n",
    "        metrics_to_log = {\"test/\" + k: v for k, v in results.items()}\n",
    "        metrics_to_log[\"test/batch_umap\"] = wandb.Image(\n",
    "            str(save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\"),\n",
    "            caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "        )\n",
    "\n",
    "        metrics_to_log[\"test/celltype_umap\"] = wandb.Image(\n",
    "            str(save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\"),\n",
    "            caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "        )\n",
    "        metrics_to_log[\"test/best_model_epoch\"] = best_model_epoch\n",
    "        wandb.log(metrics_to_log)\n",
    "        wandb.log({\"avg_bio\": results.get(\"avg_bio\", 0.0)})\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30430479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best model\n",
    "torch.save(best_model.state_dict(), save_dir / \"best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f160cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "artifact = wandb.Artifact(f\"best_model\", type=\"model\")\n",
    "glob_str = os.path.join(save_dir, \"best_model.pt\")\n",
    "artifact.add_file(glob_str)\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "run.finish()\n",
    "wandb.finish()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c5bf54-0d47-44da-a34b-4df3fac76124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
