{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Pass Encoding with scGPT TransformerModel\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load the pre-trained scGPT model and Immune Human dataset (from Tutorial_GRN)\n",
    "2. Tokenize and pad the data\n",
    "3. Create a PyTorch DataLoader\n",
    "4. Run `model._encode` to get transformer output embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load pre-trained model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chasty2/Documents/scFM_benchmarking/.venv/lib/python3.12/site-packages/scgpt/model/model.py:21: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/chasty2/Documents/scFM_benchmarking/.venv/lib/python3.12/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/chasty2/Documents/scFM_benchmarking/.venv/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/chasty2/Documents/scFM_benchmarking/.venv/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/chasty2/Documents/scFM_benchmarking/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab, tokenize_and_pad_batch\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt.utils import set_seed\n",
    "\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "n_hvg = 1200\n",
    "n_bins = 51\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "n_input_bins = n_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path(\"../models/scGPT_bc\")\n",
    "model_config_file = model_dir / \"args.json\"\n",
    "model_file = model_dir / \"best_model.pt\"\n",
    "vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "for s in special_tokens:\n",
    "    if s not in vocab:\n",
    "        vocab.append_token(s)\n",
    "\n",
    "with open(model_config_file, \"r\") as f:\n",
    "    model_configs = json.load(f)\n",
    "\n",
    "embsize = model_configs[\"embsize\"]\n",
    "nhead = model_configs[\"nheads\"]\n",
    "d_hid = model_configs[\"d_hid\"]\n",
    "nlayers = model_configs[\"nlayers\"]\n",
    "n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "\n",
    "gene2idx = vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading params encoder.embedding.weight with shape torch.Size([36574, 512])\n",
      "Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "Loading params decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "Model on cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    vocab=vocab,\n",
    "    pad_value=pad_value,\n",
    "    n_input_bins=n_input_bins,\n",
    ")\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "    print(f\"Loading all model params from {model_file}\")\n",
    "except Exception:\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = torch.load(model_file, map_location=device)\n",
    "    pretrained_dict = {\n",
    "        k: v\n",
    "        for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and v.shape == model_dict[k].shape\n",
    "    }\n",
    "    for k, v in pretrained_dict.items():\n",
    "        print(f\"Loading params {k} with shape {v.shape}\")\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"Model on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering genes by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Subsetting highly variable genes ...\n",
      "scGPT - INFO - Binning data ...\n",
      "adata shape after preprocessing: (33506, 1200)\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"../data\")\n",
    "adata = sc.read(str(data_dir / \"Immune_ALL_human.h5ad\"), cache=True)\n",
    "adata.obs[\"celltype\"] = adata.obs[\"final_annotation\"].astype(str)\n",
    "data_is_raw = False\n",
    "\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",\n",
    "    filter_gene_by_counts=3,\n",
    "    filter_cell_by_counts=False,\n",
    "    normalize_total=1e4,\n",
    "    result_normed_key=\"X_normed\",\n",
    "    log1p=data_is_raw,\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=n_hvg,\n",
    "    hvg_flavor=\"cell_ranger\",\n",
    "    binning=n_bins,\n",
    "    result_binned_key=\"X_binned\",\n",
    ")\n",
    "preprocessor(adata, batch_key=\"batch\")\n",
    "print(f\"adata shape after preprocessing: {adata.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tokenize and pad the data\n",
    "\n",
    "`tokenize_and_pad_batch` converts the binned expression matrix into:\n",
    "- **\"genes\"**: gene token IDs, shape `(n_cells, max_len)`\n",
    "- **\"values\"**: expression values, shape `(n_cells, max_len)`\n",
    "\n",
    "It appends a `<cls>` token at position 0 and pads shorter sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genes in vocab: 1173 / 1200\n",
      "Expression matrix shape: (33506, 1173)\n"
     ]
    }
   ],
   "source": [
    "# Map gene names in adata to their vocab indices\n",
    "genes_in_vocab = [g for g in adata.var.index if g in gene2idx]\n",
    "gene_ids = np.array([gene2idx[g] for g in genes_in_vocab])\n",
    "\n",
    "# Get the binned expression matrix for genes in vocab\n",
    "gene_mask = adata.var.index.isin(genes_in_vocab)\n",
    "counts = adata.layers[\"X_binned\"][:, gene_mask]\n",
    "if hasattr(counts, \"toarray\"):\n",
    "    counts = counts.toarray()\n",
    "counts = counts.astype(np.float32)\n",
    "\n",
    "print(f\"Genes in vocab: {len(genes_in_vocab)} / {adata.n_vars}\")\n",
    "print(f\"Expression matrix shape: {counts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized keys: ['genes', 'values']\n",
      "gene_ids shape: torch.Size([33506, 1174])\n",
      "values shape:   torch.Size([33506, 1174])\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = len(genes_in_vocab) + 1  # +1 for <cls> token\n",
    "\n",
    "tokenized_data = tokenize_and_pad_batch(\n",
    "    counts,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=True,\n",
    ")\n",
    "\n",
    "print(f\"Tokenized keys: {list(tokenized_data.keys())}\")\n",
    "print(f\"gene_ids shape: {tokenized_data['genes'].shape}\")\n",
    "print(f\"values shape:   {tokenized_data['values'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10000 cells\n"
     ]
    }
   ],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: dict):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"genes\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# The full dataset needs ~100GB RAM\n",
    "tokenized_subset = {key: value[:10000] for key,value in tokenized_data.items()}\n",
    "dataset = SeqDataset(tokenized_subset)\n",
    "print(f\"Dataset size: {len(dataset)} cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 157\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run `model._encode`\n",
    "\n",
    "`_encode` takes:\n",
    "- `src`: gene token IDs `(batch, seq_len)` - long tensor\n",
    "- `values`: expression values `(batch, seq_len)` - float tensor\n",
    "- `src_key_padding_mask`: boolean mask `(batch, seq_len)` - `True` where padded\n",
    "\n",
    "Returns: transformer output `(batch, seq_len, embsize)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src shape:              torch.Size([64, 1174])\n",
      "values shape:           torch.Size([64, 1174])\n",
      "padding_mask shape:     torch.Size([64, 1174])\n",
      "padding_mask % True:    0.00%\n"
     ]
    }
   ],
   "source": [
    "# Grab a single batch to demonstrate\n",
    "batch = next(iter(dataloader))\n",
    "src = batch[\"genes\"].to(device)          # (batch_size, seq_len)\n",
    "values = batch[\"values\"].to(device)      # (batch_size, seq_len)\n",
    "\n",
    "# Padding mask: True where the value equals pad_value\n",
    "src_key_padding_mask = src.eq(vocab[pad_token]).to(device)\n",
    "\n",
    "print(f\"src shape:              {src.shape}\")\n",
    "print(f\"values shape:           {values.shape}\")\n",
    "print(f\"padding_mask shape:     {src_key_padding_mask.shape}\")\n",
    "print(f\"padding_mask % True:    {src_key_padding_mask.float().mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([64, 1174, 512])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model._encode(\n",
    "        src=src,\n",
    "        values=values,\n",
    "        src_key_padding_mask=src_key_padding_mask,\n",
    "    )\n",
    "\n",
    "print(f\"Encoder output shape: {output.shape}\")  # (batch_size, seq_len, embsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS embeddings shape: torch.Size([64, 512])\n",
      "Gene embeddings shape: torch.Size([64, 1173, 512])\n"
     ]
    }
   ],
   "source": [
    "# The CLS token embedding (position 0) is the cell-level representation\n",
    "cls_embeddings = output[:, 0, :]  # (batch_size, embsize)\n",
    "print(f\"CLS embeddings shape: {cls_embeddings.shape}\")\n",
    "\n",
    "# Gene-level embeddings are at positions 1 onwards\n",
    "gene_embeddings = output[:, 1:, :]  # (batch_size, n_genes, embsize)\n",
    "print(f\"Gene embeddings shape: {gene_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Encode the full dataset\n",
    "\n",
    "Loop over all batches in the DataLoader to collect embeddings for every cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset CLS embeddings: (10000, 512)\n",
      "Cell-dependent gene embeddings: (10000, 1173, 512)\n"
     ]
    }
   ],
   "source": [
    "all_cls_embeddings = []\n",
    "cell_dep_gene_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        src = batch[\"genes\"].to(device)\n",
    "        values = batch[\"values\"].to(device)\n",
    "        src_key_padding_mask = src.eq(vocab[pad_token]).to(device)\n",
    "\n",
    "        output = model._encode(\n",
    "            src=src,\n",
    "            values=values,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "        )\n",
    "\n",
    "        # Collect CLS token embeddings\n",
    "        all_cls_embeddings.append(output[:, 0, :].cpu())\n",
    "\n",
    "        # Collect gene embedding for each cell\n",
    "        cell_dep_gene_embeddings.append(output[:, 1:, :].cpu())\n",
    "\n",
    "all_cls_embeddings = torch.cat(all_cls_embeddings, dim=0).numpy()\n",
    "cell_dep_gene_embeddings = torch.cat(cell_dep_gene_embeddings, dim=0).numpy()\n",
    "print(f\"Full dataset CLS embeddings: {all_cls_embeddings.shape}\")\n",
    "print(f\"Cell-dependent gene embeddings: {cell_dep_gene_embeddings.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scfm-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
